---
title: 카프카, 데이터 플랫폼의 최강자 3장
date: 2025-03-18
update: 2025-03-18
tags: 
	- kafka
---


## 3.1 카프카 디자인의 특징
- 데이터 파이프 라인을 통합하길 원했고, 처리량에 중점을 두고 카프카를 설계함
	- 높은 처리량과 빠른 메세지 전송, 운영 효율화를 등을 위해 **분산 시스템, 페이지 캐시, 배치 전송 처리** 등의 기능이 구현됨

### 3.1.1 분산 시스템
- 같은 역할을 하는 여러 대의 서버로 이뤄진 서버 그룹을 분산 시스템이라고 함
	- 단일 시스템보다 더 높은 성능
	- 하나의 서버 또는 노드가 장애가 발생하면 다른 서버 또는 노드가 대신 처리
	- 시스템 확장이 용이
- 동일한 역할을 하는 추가해 부하를 분산할 수 있는 것은 장점이나 불필요하게 서버만 추가하면 비용이 증가하는 단점도 존재.

### 3.1.2 페이지 캐시
- 높은 처리량을 위해 추가됨
- OS는 잔여 물리적 메모리의 일부를 페이지 캐시로 유지해 OS의 전체적인 성능 향상을 도모
- 디스크에 읽고 쓰기를 하지 않고 페이지 캐시를 통해 읽고 쓰는 방식을 사용하면 처리속도가 매우 빠르기 때문에 전체적인 성능 향상
-> 그래서 언제 사용?
-> 메세지를 디스크에 저장하던데 그 얘기 인가?

### 3.1.3 배치 전송 처리
- 작은  I/O가 빈번하게 일어나면 속도 저하의 원인이 될 수 있기에 작은 I/O 작업들을 묶어서 처리할 수 있도록 배치 작업으로 처리
- 배치 전송 시 네트워크 왕복 오버헤드 등을 줄이게 되어 속도 향상에 매우 큰 모움이 됨

## 3.2 카프카 데이터 모델
토픽: 메세지를 받을 수 있도록 논리적으로 묶은 개념
파티션: 토픽을 구성하는 데이터 저장소로서 수평 확장이 가능한 단위

### 3.2.1 토픽의 이해
- 카프카 클러스터는 토픽이 불리는 곳에 데이터를 저장
	- ex) 메일 시스템에서 토픽은 메일 주소라고 생각할 수 있음
	- 메일을 보내는데 메일 주소가 없다면?
- 일종의 데이터를 구분하기 위한 단위
- 토픽 이름은 249자 미만으로 영문, 숫자, '.', '_', '-' 를 조합할 수 있음

### 3.2.2 파티션의 이해
- 파티션은 토픽을 분할한 것, WHY?
- 빠른 전송을 위해 토픽의 파티션을 늘리고 그 수만큼 프로듀서 수도 늘려야 함
	- 4개 메세지를 하나의 토픽으로 전송할 때 
		- 직렬: 1초씩 4번 -> 총 4초
		- 병렬: 1초씩 1번 -> 총 1초 (조건: 메세지의 순서가 보장되어야 함 -> 이전 메세지 처리가 완료된 후 다음 메세지를 처리하게 됨)
- 그럼 무조건 파티션의 수를 늘려야 하는가?
	- 언제나 무조건 늘리는 것은 좋지 않다. 경험상
	 1. 파일 핸들러의 낭비
		 - 파티션은 브로커의 디렉토리와 매핑되고 데이터 마다 2개의 파일이 존재 (인덱스와 실제 데이터) 
		 - 파티션 증가 -> 파일 핸들러 수 증가 => 리소스 낭비
	 2. 장애 복구 시간 증가
		 - 브로커 다운 시 해당 브로커에 리더가 있는 파티션은 일시적으로 사용 불가
		 - 리더를 팔로워 브로커로 이동시켜 클라이언트 요청을 처리할 수 있게 함 -> 컨트롤러로 지정된 브로커가 처리
		 - 예를 들어 1000개의 파티션이 있고 2개의 리플리케이션이 있는 브로커가 갑자기 종료되면 해당 브로커에 있는 1000개의 파티션은 사용불가
		 - 만약 컨트롤러가 각 파티션별로 새로운 리더를 선출하는데 5밀리초가 걸리면 최종 5초가 소요됨
		 - 추가로 컨트롤러 브로커가 종료되면 새로운 리더 선출할 수 없음
		 - 컨트롤러 페일오버는 자동이지만 주키퍼에서 모든 파티션의 데이터를 읽어야 함
		 - 따라서 처리량 기준으로 파티션 수를 정해야 함
		 - 참고로 파티션을 줄이는 방법은 없으니 필요하다면 삭제를 해야함

### 3.2.3 오프셋과 메세지 순서
- 오프셋
	- 각 파티션마다 메세지가 저장되는 위치
	- 파티션 내에서 유일하고 순차적으로 증가하는 숫자(64비트) 형태

## 3.3 카프카의 고가용성과 리플리케이션
- 토픽 자체를 리플리케이션 하는 것이 아니라 포틱을 이루는 각각의 파티션을 리플리케이션하는 것

### 3.3.1 리플리케이션 팩터와 리더, 팔로워의 역할
- 리플리케이션 팩터
	- 카프카 내 몇 개의 리플리케이션을 유지하겠다는 의미
	- 기본 값 설정은 1
	- 설정 파일에서 추가 및 수정이 가능
	- 클러스터 내 모든 브로거에 동일하게 설정해야 함
	- 원본이 리더, 복제본이 팔로워
	- 모든 읽기와 쓰기는 리더를 통해서만 이루어짐
	- 팔로워는 읽기와 쓰기에 관여하지 않음
		- RDS active standby 와 비슷한 동작인가
	- 리더가 다운 시 팔로워가 새로운 리더가 되어 프로듀서의 요청에 응답
	- 단점
		- 토픽 사이즈 * 리플리케이션 팩터 만큼의 저장소가  필요
		- 비활성화된 토픽의 상태 및 리플리케이션 상태 체크를 위해 브로커의 리소스 사용량 증가

### 3.3.2 리더와 팔로워의 관리
- 모든 읽기와 쓰기는 리더를 통해 이루어지고, 팔로워는 리더를 주기적으로 보면서 없는 데이터를 가져오는 방식
- 정합성이 맞지 않은 채로 새로운 리더로 승격되면 큰 문제가 발생할 수 있음
- ISR (In Sync Replication): 현재 리플리케이션 되고 있는 그룹 -> 이 그룹 안에 있어야 새로운 리더로 승격 가능
- ISR 축소 동작
	- 팔로워들은 매우 짧은 주기로 리더를 확인
	- 리더는 팔로워들이 주기적으로 데이터를 확인을 하고 있는지 확인하여, 설정된 일정 주기만큼 확인 요청이 오지 않는다면 팔로워의 이상을 감지하고 ISR에서 추방

## 3.4 모든 브로커가 다운된다면
- 카프카 클러스터 내 모든 브로커가 다운되는 상황은 비번한 상황은 아니지만 일어날 수 있는 일
- 모든 브로커가 다운되면 프로듀서가 더 이상 리더가 존재하지 않아 메세지를 보낼 수 없음
	1. 마지막 리더가 살아나길 기다리거나 => 일관성
		- 메세지 손실 없이 장애 상황을 넘길 수 있어보이지만 해당 브로커가 알 수 없는 이유로 살아나지 못할 수 있음
		- 이 경우 정상화될 때까지 장애 상황이 길어짐
	2. ISR에서 추방되었지만 먼저 살아나면 자동으로 리더가 됨 => 가용성
		- 마지막 리더가 아니라면 메세지 손실이 발생
		- 새로운 리더(마지막 리더 x)가 있을 때 올드 리더가 정상화되면 리더와 동기화 작업을 하며 메세지 손실
		- 그치만 브로커 하나라도 살아나면 서비스 역시 빠르게 정상화가 가능

--- 
## 찾아볼 것
### Q1. ISR에서 리더가 비정상 팔로워를 탐지하는 것은 단순히 복제 요청의 주기만으로 판단하나?
- 리더는 특정 팔로워가 특정 지기의 시간만큼 복제 요청을 하지 않으면 리플리케이션 동작에 문제가 발생했다고 판단하여 추방
- ISR그룹 내에서 모든 팔로워에게 복제가 완료되었다면 커밋되었다는 표시를 하며 마지막 커밋 offset 위치는 high water mark라고 부름
- log-end-offset은 해당 토픽 파티션에 저장된 데이터의 끝을 나타내며(브로커가 관리), 파티션에 쓰고 클러스터에 커밋된 마지막 메세지의 오프셋
	- LAG는 컨슈머 current-offset과 브로커의 log-end-offset의 차이로 만들어짐
- 커밋된 메세지만 컨슈머가 읽어갈 수 있음
- 브로커는 재시작 시 커밋된 메세지를 유지하기 위해 로컬 디스크에 replication-offset-checkpoint라는 파일에 마지막 커밋 offset 위치를 저장
- 특정 토픽의 파티션이 복제되지 않거나 문제가 있다고 판단되면 replication-offset-checkpoint 파일의 내용을 확인하여 다른 브로커와 비교해보면 어떤 브로커, 파티션에 문제가 있는지 파악 가능

### Q2. ISR과 관련된 정보는 어디에 저장되며, 브로커가 모두 다운되었다가 재시작되면 해당 브로커가 ISR에 있는지 아닌지는 어떻게 알지? 각 브로커 스스로 알고 있나?
- 찾아보기

---
출처 : https://colevelup.tistory.com/19